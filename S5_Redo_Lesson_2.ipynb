{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09499009-ec23-4945-ae91-ffc2db15b615",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Modelo Base tutorial\n",
    "\n",
    "https://www.kaggle.com/code/falrrema/exercise-trend/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09608559-fd92-4917-8a21-08c945c0e276",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Seteando entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9072aad-bd73-4042-86c4-35c512d4d187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Funciones claves\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "def load_install_package(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            importlib.import_module(package)\n",
    "            print(f\"{package} está instalada y lista para usar.\")\n",
    "        except ImportError:\n",
    "            print(f\"{package} no está instalada. Instalando...\")\n",
    "            subprocess.check_call(['pip', 'install', package])\n",
    "            print(f\"{package} ha sido instalada exitosamente.\")\n",
    "\n",
    "# Clase para hacer CV en bloques (creado con chatGPT)\n",
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, train_size=None, test_size=None, n_splits=None, step=None):\n",
    "        self.train_size = train_size\n",
    "        self.test_size = test_size\n",
    "        self.n_splits = n_splits\n",
    "        self.step = step if step else test_size  # Default step size is test_size\n",
    "\n",
    "    def get_n_splits(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # If train_size, test_size, and step are provided\n",
    "        if self.train_size and self.test_size:\n",
    "            max_splits = 1 + (n_samples - self.train_size - self.test_size) // self.step\n",
    "            if self.n_splits:\n",
    "                if self.n_splits > max_splits:\n",
    "                    raise ValueError(f\"Cannot have n_splits > {max_splits} with the provided train and test sizes and step. Consider reducing n_splits.\")\n",
    "                return self.n_splits\n",
    "            return max_splits\n",
    "        \n",
    "        # If only n_splits is provided\n",
    "        if self.n_splits:\n",
    "            return self.n_splits\n",
    "        raise ValueError(\"Either train/test sizes or n_splits should be provided.\")\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        n_splits = self.get_n_splits(X)\n",
    "        \n",
    "        if self.train_size and self.test_size:\n",
    "            for i in range(n_splits):\n",
    "                start = i * self.step\n",
    "                mid = start + self.train_size\n",
    "                stop = mid + self.test_size\n",
    "                if stop > n_samples:\n",
    "                    stop = n_samples\n",
    "                yield indices[start: mid], indices[mid: stop]\n",
    "        else:\n",
    "            k_fold_size = n_samples // n_splits\n",
    "            for i in range(n_splits):\n",
    "                start = i * k_fold_size\n",
    "                stop = start + k_fold_size\n",
    "                mid = int(0.5 * (stop - start)) + start\n",
    "                yield indices[start: mid], indices[mid: stop]\n",
    "\n",
    "# Función para hacer CV generando una lista de DF con splits de train y test\n",
    "def sliding_period(df, btss, period):\n",
    "    # Step 1: Check Period Index\n",
    "    if period == \"day\":\n",
    "        if not isinstance(df.index, pd.PeriodIndex) or df.index.freqstr != 'D':\n",
    "            raise ValueError(\"Index should be of type PeriodIndex with 'D' frequency for 'day' period.\")\n",
    "    elif period == \"week\":\n",
    "        if not isinstance(df.index, pd.PeriodIndex) or df.index.freqstr != 'W-SUN':\n",
    "            raise ValueError(\"Index should be of type PeriodIndex with 'W-SUN' frequency for 'week' period.\")\n",
    "    elif period == \"month\":\n",
    "        if not isinstance(df.index, pd.PeriodIndex) or df.index.freqstr != 'M':\n",
    "            raise ValueError(\"Index should be of type PeriodIndex with 'M' frequency for 'month' period.\")\n",
    "    elif period == \"quarter\":\n",
    "        if not isinstance(df.index, pd.PeriodIndex) or df.index.freqstr != 'Q-DEC':\n",
    "            raise ValueError(\"Index should be of type PeriodIndex with 'Q-DEC' frequency for 'quarter' period.\")\n",
    "    elif period == \"year\":\n",
    "        if not isinstance(df.index, pd.PeriodIndex) or df.index.freqstr != 'A-DEC':\n",
    "            raise ValueError(\"Index should be of type PeriodIndex with 'A-DEC' frequency for 'year' period.\")\n",
    "    # Add more checks for other periods if needed\n",
    "\n",
    "    # Step 2: Extract Unique Periods\n",
    "    unique_periods = df.index.unique()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Step 3 & 4: Apply `BlockingTimeSeriesSplit` and Filter & Annotate DataFrame\n",
    "    for train_periods, test_periods in btss.split(unique_periods):\n",
    "        train_df = df[df.index.isin(unique_periods[train_periods])].copy()\n",
    "        train_df['split'] = 'train'\n",
    "        \n",
    "        test_df = df[df.index.isin(unique_periods[test_periods])].copy()\n",
    "        test_df['split'] = 'test'\n",
    "        \n",
    "        combined_df = pd.concat([train_df, test_df])\n",
    "        results.append(combined_df)\n",
    "\n",
    "    # Step 5: Return List\n",
    "    return results\n",
    "  \n",
    "# Funcion de chequeo de BlockingTimeSeriesSplit para experimentar splits\n",
    "def check_BlockTimeSeriesSplit(df, train_size=None, test_size=None, n_splits=None, step=None):\n",
    "    # Extract unique periods\n",
    "    unique_periods = df.index.unique()\n",
    "    \n",
    "    # Calculate max_splits for train_size and test_size\n",
    "    max_splits = 1 + (len(unique_periods) - train_size - test_size) // (step or test_size)\n",
    "    \n",
    "    # Validate and compute missing parameters\n",
    "    if n_splits:\n",
    "        if train_size is None and test_size is None:\n",
    "            split_size = len(unique_periods) // n_splits\n",
    "            train_size = split_size - (split_size // 3)\n",
    "            test_size = split_size // 3\n",
    "        elif train_size and test_size:\n",
    "            if n_splits > max_splits:\n",
    "                raise ValueError(f\"Cannot have n_splits > {max_splits}. Adjust train/test sizes or step.\")\n",
    "        else:\n",
    "            raise ValueError(\"If n_splits is provided along with train_size or test_size, both train_size and test_size must be provided.\")\n",
    "    elif train_size and test_size:\n",
    "        n_splits = max_splits\n",
    "    else:\n",
    "        raise ValueError(\"Provide either n_splits or both train_size and test_size.\")\n",
    "    \n",
    "    # Create an instance of BlockingTimeSeriesSplit\n",
    "    btss = BlockingTimeSeriesSplit(train_size=train_size, test_size=test_size, n_splits=n_splits, step=step or test_size)\n",
    "    \n",
    "    # Collect split details\n",
    "    results = []\n",
    "    fold_number = 1\n",
    "    last_test_period = None\n",
    "    for train_periods, test_periods in btss.split(unique_periods):\n",
    "        results.append([fold_number, 'Train', len(train_periods), unique_periods[train_periods][0], unique_periods[train_periods][-1]])\n",
    "        results.append([fold_number, 'Test', len(test_periods), unique_periods[test_periods][0], unique_periods[test_periods][-1]])\n",
    "        last_test_period = unique_periods[test_periods][-1]\n",
    "        fold_number += 1\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    splits_df = pd.DataFrame(results, columns=['Fold', 'Split', 'Length', 'Initial_period', 'Ending_period'])\n",
    "    \n",
    "    # Calculate unused periods\n",
    "    unused_periods = len(unique_periods[unique_periods.tolist().index(last_test_period)+1:])\n",
    "    \n",
    "    # Print report\n",
    "    print(f\"Dataframe Initial Period: {unique_periods[0]}\")\n",
    "    print(f\"Dataframe Max Period: {unique_periods[-1]}\")\n",
    "    print(f\"Maximum possible splits: {max_splits}\")\n",
    "    if n_splits:\n",
    "        print(f\"n_splits: {n_splits}\")\n",
    "    print(f\"Number of unused periods: {unused_periods}\")\n",
    "    \n",
    "    return splits_df\n",
    "\n",
    "# Funcion de medicion\n",
    "def RMSLE(y_true: list, y_pred: list) -> float:\n",
    "    \"\"\"\n",
    "    The Root Mean Squared Log Error (RMSLE) metric using only NumPy\n",
    "    \n",
    "    :param y_true: The ground truth labels given in the dataset\n",
    "    :param y_pred: Our predictions\n",
    "    :return: The RMSLE score\n",
    "    \"\"\"\n",
    "    n = len(y_true)\n",
    "    msle = np.sqrt(np.mean(np.square(np.log1p(y_pred) - np.log1p(y_true))))\n",
    "    return msle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d562a50f-ae38-4976-9865-91a1bd1f9b68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy está instalada y lista para usar.\npandas está instalada y lista para usar.\nmatplotlib está instalada y lista para usar.\npathlib está instalada y lista para usar.\nseaborn está instalada y lista para usar.\nsklearn está instalada y lista para usar.\nstatsmodels está instalada y lista para usar.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)\n",
       "\u001B[0;32m<command-288323088679126>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[0mlibrerias\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'numpy'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pandas'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'matplotlib'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pathlib'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'seaborn'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'sklearn'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'statsmodels'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'kaggle'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 2\u001B[0;31m \u001B[0mload_install_package\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlibrerias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m<command-288323088679125>\u001B[0m in \u001B[0;36mload_install_package\u001B[0;34m(packages)\u001B[0m\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mpackage\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpackages\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 8\u001B[0;31m             \u001B[0mimportlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimport_module\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpackage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      9\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{package} está instalada y lista para usar.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     10\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/usr/lib/python3.9/importlib/__init__.py\u001B[0m in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n",
       "\u001B[1;32m    125\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    126\u001B[0m             \u001B[0mlevel\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 127\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_bootstrap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gcd_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpackage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n",
       "\n",
       "\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n",
       "\n",
       "\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n",
       "\n",
       "\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n",
       "\n",
       "\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap_external.py\u001B[0m in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n",
       "\n",
       "\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n",
       "\n",
       "\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-6114644e-a677-4ba3-830e-a73d06e733e5/lib/python3.9/site-packages/kaggle/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[0mapi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKaggleApi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mApiClient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 23\u001B[0;31m \u001B[0mapi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauthenticate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-6114644e-a677-4ba3-830e-a73d06e733e5/lib/python3.9/site-packages/kaggle/api/kaggle_api_extended.py\u001B[0m in \u001B[0;36mauthenticate\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    401\u001B[0m                 \u001B[0mconfig_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_config_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    402\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 403\u001B[0;31m                 raise IOError('Could not find {}. Make sure it\\'s located in'\n",
       "\u001B[0m\u001B[1;32m    404\u001B[0m                               ' {}. Or use the environment method.'.format(\n",
       "\u001B[1;32m    405\u001B[0m                                   self.config_file, self.config_dir))\n",
       "\n",
       "\u001B[0;31mOSError\u001B[0m: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)\n\u001B[0;32m<command-288323088679126>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mlibrerias\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'numpy'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pandas'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'matplotlib'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pathlib'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'seaborn'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'sklearn'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'statsmodels'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'kaggle'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mload_install_package\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlibrerias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m<command-288323088679125>\u001B[0m in \u001B[0;36mload_install_package\u001B[0;34m(packages)\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mpackage\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpackages\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m             \u001B[0mimportlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimport_module\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpackage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m             \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{package} está instalada y lista para usar.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mImportError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/usr/lib/python3.9/importlib/__init__.py\u001B[0m in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    126\u001B[0m             \u001B[0mlevel\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 127\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_bootstrap\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_gcd_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlevel\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpackage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    128\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_gcd_import\u001B[0;34m(name, package, level)\u001B[0m\n\n\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_find_and_load\u001B[0;34m(name, import_)\u001B[0m\n\n\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_find_and_load_unlocked\u001B[0;34m(name, import_)\u001B[0m\n\n\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_load_unlocked\u001B[0;34m(spec)\u001B[0m\n\n\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap_external.py\u001B[0m in \u001B[0;36mexec_module\u001B[0;34m(self, module)\u001B[0m\n\n\u001B[0;32m/usr/lib/python3.9/importlib/_bootstrap.py\u001B[0m in \u001B[0;36m_call_with_frames_removed\u001B[0;34m(f, *args, **kwds)\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-6114644e-a677-4ba3-830e-a73d06e733e5/lib/python3.9/site-packages/kaggle/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0mapi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKaggleApi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mApiClient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m \u001B[0mapi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauthenticate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-6114644e-a677-4ba3-830e-a73d06e733e5/lib/python3.9/site-packages/kaggle/api/kaggle_api_extended.py\u001B[0m in \u001B[0;36mauthenticate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    401\u001B[0m                 \u001B[0mconfig_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_config_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    402\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 403\u001B[0;31m                 raise IOError('Could not find {}. Make sure it\\'s located in'\n\u001B[0m\u001B[1;32m    404\u001B[0m                               ' {}. Or use the environment method.'.format(\n\u001B[1;32m    405\u001B[0m                                   self.config_file, self.config_dir))\n\n\u001B[0;31mOSError\u001B[0m: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.",
       "errorSummary": "<span class='ansi-red-fg'>OSError</span>: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "librerias = ['numpy', 'pandas', 'matplotlib', 'pathlib', 'seaborn', 'sklearn', 'statsmodels', 'kaggle']\n",
    "load_install_package(librerias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04fd1802-024a-45e3-8f46-b6d619f898f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)\n",
       "\u001B[0;32m<command-288323088679127>\u001B[0m in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mkaggle\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear_model\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mLinearRegression\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mstatsmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtsa\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeterministic\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mCalendarFourier\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDeterministicProcess\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-6114644e-a677-4ba3-830e-a73d06e733e5/lib/python3.9/site-packages/kaggle/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     22\u001B[0m \u001B[0mapi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKaggleApi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mApiClient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 23\u001B[0;31m \u001B[0mapi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauthenticate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\n",
       "\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-6114644e-a677-4ba3-830e-a73d06e733e5/lib/python3.9/site-packages/kaggle/api/kaggle_api_extended.py\u001B[0m in \u001B[0;36mauthenticate\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    401\u001B[0m                 \u001B[0mconfig_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_config_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    402\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 403\u001B[0;31m                 raise IOError('Could not find {}. Make sure it\\'s located in'\n",
       "\u001B[0m\u001B[1;32m    404\u001B[0m                               ' {}. Or use the environment method.'.format(\n",
       "\u001B[1;32m    405\u001B[0m                                   self.config_file, self.config_dir))\n",
       "\n",
       "\u001B[0;31mOSError\u001B[0m: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)\n\u001B[0;32m<command-288323088679127>\u001B[0m in \u001B[0;36m<cell line: 6>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mkaggle\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear_model\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mLinearRegression\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mstatsmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtsa\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeterministic\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mCalendarFourier\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDeterministicProcess\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py\u001B[0m in \u001B[0;36mimport_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    169\u001B[0m             \u001B[0;31m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    170\u001B[0m             \u001B[0;31m# look at preceding stack frames for relevant error information.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 171\u001B[0;31m             \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpython_builtin_import\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mglobals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlocals\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfromlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlevel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    172\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mis_root_import\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mthread_local\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nest_level\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-6114644e-a677-4ba3-830e-a73d06e733e5/lib/python3.9/site-packages/kaggle/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0mapi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mKaggleApi\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mApiClient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m \u001B[0mapi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauthenticate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-6114644e-a677-4ba3-830e-a73d06e733e5/lib/python3.9/site-packages/kaggle/api/kaggle_api_extended.py\u001B[0m in \u001B[0;36mauthenticate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    401\u001B[0m                 \u001B[0mconfig_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_config_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig_data\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    402\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 403\u001B[0;31m                 raise IOError('Could not find {}. Make sure it\\'s located in'\n\u001B[0m\u001B[1;32m    404\u001B[0m                               ' {}. Or use the environment method.'.format(\n\u001B[1;32m    405\u001B[0m                                   self.config_file, self.config_dir))\n\n\u001B[0;31mOSError\u001B[0m: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.",
       "errorSummary": "<span class='ansi-red-fg'>OSError</span>: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargamos las librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import kaggle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62be1d20-eb61-409a-94e5-ae7eccbc79c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cargando data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0953340d-f68b-420b-92cc-c6db59a880ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargando feriados\n",
    "holidays_events = spark.sql('select * from analytics_inversiones.kgl_holidays_events').toPandas()\n",
    "# Establecer el tipo de datos de cada columna\n",
    "# Definir el diccionario de tipos de datos por columna\n",
    "dtype={\n",
    "        'date' : 'datetime64[ns]',\n",
    "        'type': 'category',\n",
    "        'locale': 'category',\n",
    "        'locale_name': 'category',\n",
    "        'description': 'category',\n",
    "        'transferred': 'bool',\n",
    "    }\n",
    "holidays_events = holidays_events.astype(dtype)\n",
    "holidays_events = holidays_events.set_index('date').to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c6398ff-e2b5-4390-b9ad-377b84806c34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargando Train\n",
    "store_sales = spark.sql('select * from analytics_inversiones.kgl_train').toPandas()\n",
    "# Establecer el tipo de datos de cada columna\n",
    "# Definir el diccionario de tipos de datos por columna\n",
    "dtype={ \n",
    "       'date': 'datetime64[ns]',\n",
    "        'store_nbr': 'category',\n",
    "        'family': 'category',\n",
    "        'sales': 'float32'\n",
    "}\n",
    "store_sales = store_sales.astype(dtype)\n",
    "store_sales['date'] = store_sales.date.dt.to_period('D')\n",
    "store_sales = store_sales.set_index(['date']).sort_index()\n",
    "#store_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd8b235-fa5a-43a4-b81c-f42f99e52864",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargando Test\n",
    "df_test = spark.sql('select * from analytics_inversiones.kgl_test').toPandas()\n",
    "# Establecer el tipo de datos de cada columna\n",
    "# Definir el diccionario de tipos de datos por columna\n",
    "dtype={ \n",
    "       'date': 'datetime64[ns]',\n",
    "        'store_nbr': 'category',\n",
    "        'family': 'category',\n",
    "        'onpromotion': 'uint32',\n",
    "}\n",
    "df_test = df_test.astype(dtype)\n",
    "df_test['date'] = df_test.date.dt.to_period('D')\n",
    "df_test = df_test.set_index(['date']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62a002c-d476-4fe8-afff-2ded35640c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Generando tablones CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ca4e8cf-12a9-42f7-b3d7-21f2a515501e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Fabrique 3 funciones:\n",
    "\n",
    "**BlockingTimeSeriesSplit**: La clase que permite la fabricación del generador de splits que puede recibir \n",
    "  `n_splits` = número de splits train y test que se quiere hacer\n",
    "  `train_size` = el tamaño de train en periodos\n",
    "  `test_size` = el tamaño de test en periodos\n",
    "  `step` = cuantos periodos se salta ante el siguiente splis, default es el tamaño del test\n",
    "\n",
    "**check_BlockTimeSeriesSplit**: Permite experimentar los splits que se quiere hacer, te da un pequeño informe basado en las métricas que se les pasa.\n",
    "\n",
    "**sliding_period**: Implementa los splits en el dataframe que se le pase. Exige que el dataframe tenga un indice de fecha como *PeriodIndex*. Devuelve una lista de dataframes de cada splits con una columna que diferencia el train del test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4387ceb-b905-49af-81a0-633991b5f1d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Initial Period: 2013-01-01\nDataframe Max Period: 2017-08-15\nMaximum possible splits: 14\nn_splits: 14\nNumber of unused periods: 74\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Split</th>\n",
       "      <th>Length</th>\n",
       "      <th>Initial_period</th>\n",
       "      <th>Ending_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2013-04-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-04-11</td>\n",
       "      <td>2013-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2013-04-26</td>\n",
       "      <td>2013-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-08-04</td>\n",
       "      <td>2013-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2013-08-19</td>\n",
       "      <td>2013-11-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2013-11-27</td>\n",
       "      <td>2013-12-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2013-12-12</td>\n",
       "      <td>2014-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2014-03-23</td>\n",
       "      <td>2014-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2014-04-07</td>\n",
       "      <td>2014-07-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2014-07-16</td>\n",
       "      <td>2014-07-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2014-07-31</td>\n",
       "      <td>2014-11-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2014-11-08</td>\n",
       "      <td>2014-11-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2014-11-23</td>\n",
       "      <td>2015-03-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-03-04</td>\n",
       "      <td>2015-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2015-03-19</td>\n",
       "      <td>2015-06-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-06-27</td>\n",
       "      <td>2015-07-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2015-07-12</td>\n",
       "      <td>2015-10-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2015-10-20</td>\n",
       "      <td>2015-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2015-11-04</td>\n",
       "      <td>2016-02-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2016-02-13</td>\n",
       "      <td>2016-02-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2016-02-28</td>\n",
       "      <td>2016-06-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2016-06-07</td>\n",
       "      <td>2016-06-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>2016-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>2016-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2016-10-15</td>\n",
       "      <td>2017-01-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2017-01-24</td>\n",
       "      <td>2017-02-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14</td>\n",
       "      <td>Train</td>\n",
       "      <td>100</td>\n",
       "      <td>2017-02-08</td>\n",
       "      <td>2017-05-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14</td>\n",
       "      <td>Test</td>\n",
       "      <td>15</td>\n",
       "      <td>2017-05-19</td>\n",
       "      <td>2017-06-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Fold</th>\n      <th>Split</th>\n      <th>Length</th>\n      <th>Initial_period</th>\n      <th>Ending_period</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2013-01-01</td>\n      <td>2013-04-10</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2013-04-11</td>\n      <td>2013-04-25</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2013-04-26</td>\n      <td>2013-08-03</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2013-08-04</td>\n      <td>2013-08-18</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2013-08-19</td>\n      <td>2013-11-26</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2013-11-27</td>\n      <td>2013-12-11</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2013-12-12</td>\n      <td>2014-03-22</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>4</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2014-03-23</td>\n      <td>2014-04-06</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>5</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2014-04-07</td>\n      <td>2014-07-15</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>5</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2014-07-16</td>\n      <td>2014-07-30</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>6</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2014-07-31</td>\n      <td>2014-11-07</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>6</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2014-11-08</td>\n      <td>2014-11-22</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>7</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2014-11-23</td>\n      <td>2015-03-03</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2015-03-04</td>\n      <td>2015-03-18</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>8</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2015-03-19</td>\n      <td>2015-06-26</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>8</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2015-06-27</td>\n      <td>2015-07-11</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>9</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2015-07-12</td>\n      <td>2015-10-19</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>9</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2015-10-20</td>\n      <td>2015-11-03</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>10</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2015-11-04</td>\n      <td>2016-02-12</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>10</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2016-02-13</td>\n      <td>2016-02-27</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>11</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2016-02-28</td>\n      <td>2016-06-06</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>11</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2016-06-07</td>\n      <td>2016-06-21</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>12</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2016-06-22</td>\n      <td>2016-09-29</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>12</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2016-09-30</td>\n      <td>2016-10-14</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>13</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2016-10-15</td>\n      <td>2017-01-23</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>13</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2017-01-24</td>\n      <td>2017-02-07</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>14</td>\n      <td>Train</td>\n      <td>100</td>\n      <td>2017-02-08</td>\n      <td>2017-05-18</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>14</td>\n      <td>Test</td>\n      <td>15</td>\n      <td>2017-05-19</td>\n      <td>2017-06-02</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experimento con los splits para lograr abordar todo el dataset idealmente 10 splits\n",
    "ventana_train = 100\n",
    "ventana_test = 15\n",
    "df_check = check_BlockTimeSeriesSplit(df = store_sales, train_size=ventana_train, test_size=ventana_test, n_splits=None, step=ventana_train+ventana_test)\n",
    "df_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef21cba9-c4ad-49d2-8e0b-e3c8ed835fcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-25</th>\n",
       "      <td>204925</td>\n",
       "      <td>9</td>\n",
       "      <td>POULTRY</td>\n",
       "      <td>395.60199</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-25</th>\n",
       "      <td>204926</td>\n",
       "      <td>9</td>\n",
       "      <td>PREPARED FOODS</td>\n",
       "      <td>65.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-25</th>\n",
       "      <td>204927</td>\n",
       "      <td>9</td>\n",
       "      <td>PRODUCE</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-25</th>\n",
       "      <td>204928</td>\n",
       "      <td>9</td>\n",
       "      <td>SCHOOL AND OFFICE SUPPLIES</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-25</th>\n",
       "      <td>204929</td>\n",
       "      <td>9</td>\n",
       "      <td>SEAFOOD</td>\n",
       "      <td>22.25300</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204930 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>store_nbr</th>\n      <th>family</th>\n      <th>sales</th>\n      <th>onpromotion</th>\n      <th>split</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2013-01-01</th>\n      <td>0</td>\n      <td>1</td>\n      <td>AUTOMOTIVE</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2013-01-01</th>\n      <td>1</td>\n      <td>1</td>\n      <td>BABY CARE</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2013-01-01</th>\n      <td>2</td>\n      <td>1</td>\n      <td>BEAUTY</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2013-01-01</th>\n      <td>3</td>\n      <td>1</td>\n      <td>BEVERAGES</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>2013-01-01</th>\n      <td>4</td>\n      <td>1</td>\n      <td>BOOKS</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>train</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2013-04-25</th>\n      <td>204925</td>\n      <td>9</td>\n      <td>POULTRY</td>\n      <td>395.60199</td>\n      <td>0</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>2013-04-25</th>\n      <td>204926</td>\n      <td>9</td>\n      <td>PREPARED FOODS</td>\n      <td>65.00000</td>\n      <td>0</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>2013-04-25</th>\n      <td>204927</td>\n      <td>9</td>\n      <td>PRODUCE</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>2013-04-25</th>\n      <td>204928</td>\n      <td>9</td>\n      <td>SCHOOL AND OFFICE SUPPLIES</td>\n      <td>0.00000</td>\n      <td>0</td>\n      <td>test</td>\n    </tr>\n    <tr>\n      <th>2013-04-25</th>\n      <td>204929</td>\n      <td>9</td>\n      <td>SEAFOOD</td>\n      <td>22.25300</td>\n      <td>0</td>\n      <td>test</td>\n    </tr>\n  </tbody>\n</table>\n<p>204930 rows × 6 columns</p>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Genero los splits\n",
    "ventana_train = 100\n",
    "ventana_test = 15\n",
    "btss = BlockingTimeSeriesSplit(train_size=ventana_train, test_size=ventana_test, step=ventana_train + ventana_test)\n",
    "df_cv = sliding_period(df = store_sales, btss = btss, period = \"day\")\n",
    "df_cv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80596dca-bd8b-487d-b3a0-11d1f6f2bcef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Armando modelo de seasonality\n",
    "\n",
    "Armaré una funcion que hace CV para cada splits armado arriba y le aplicará una regresión lineal a los features construidos en el notebook del tutorial. Estos son primordialmente features de seasonality y uno de feriado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7827834b-148c-4da5-a54e-7dd177ac2e6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cv_predictions(splits):\n",
    "    results = []\n",
    "    rmsle_reports = []\n",
    "    \n",
    "    for i, split_df in enumerate(df_cv, start=1):\n",
    "        # Separate out train and test sets\n",
    "        train_df = split_df[split_df[\"split\"] == \"train\"].drop(columns=['id', 'onpromotion', 'split'])\n",
    "        test_df = split_df[split_df[\"split\"] == \"test\"].drop(columns=['id', 'onpromotion', 'split'])\n",
    "\n",
    "        # Reset index for train and test\n",
    "        y_train = (\n",
    "            train_df.reset_index()\n",
    "            .set_index([\"store_nbr\", \"family\", \"date\"])\n",
    "            .sort_index()\n",
    "            .unstack([\"store_nbr\", \"family\"])\n",
    "        )\n",
    "        y_test = (\n",
    "            test_df.reset_index()\n",
    "            .set_index([\"store_nbr\", \"family\", \"date\"])\n",
    "            .sort_index()\n",
    "            .unstack([\"store_nbr\", \"family\"])\n",
    "        )\n",
    "\n",
    "        # Generate deterministic features for training set\n",
    "        fourier = CalendarFourier(freq=\"M\", order=4)\n",
    "        dp = DeterministicProcess(\n",
    "            index=y_train.index,\n",
    "            constant=True,\n",
    "            order=1,\n",
    "            seasonal=True,\n",
    "            additional_terms=[fourier],\n",
    "            drop=True,\n",
    "        )\n",
    "        X_train = dp.in_sample()\n",
    "        X_train[\"NewYear\"] = X_train.index.dayofyear == 1\n",
    "\n",
    "        # Fit the linear regression model on training data\n",
    "        model = LinearRegression(fit_intercept=False)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the training data\n",
    "        y_train_pred = pd.DataFrame(\n",
    "            model.predict(X_train), index=X_train.index, columns=y_train.columns\n",
    "        )\n",
    "        y_train_pred = y_train_pred.clip(lower=0)  # Set negative predictions to zero\n",
    "\n",
    "        # Generate deterministic features for test set\n",
    "        X_test = dp.out_of_sample(steps=len(y_test))\n",
    "        X_test[\"NewYear\"] = X_test.index.dayofyear == 1\n",
    "\n",
    "        # Predict on the test data\n",
    "        y_test_pred = pd.DataFrame(\n",
    "            model.predict(X_test), index=X_test.index, columns=y_test.columns\n",
    "        )\n",
    "        y_test_pred = y_test_pred.clip(lower=0)  # Set negative predictions to zero\n",
    "\n",
    "        # Prepare final dataframes for train and test\n",
    "        train_df = pd.concat([y_train.stack([\"store_nbr\", \"family\"]), y_train_pred.stack([\"store_nbr\", \"family\"])],axis=1)\n",
    "        train_df.columns = [\"sales\", \"predicted\"]\n",
    "        train_df[\"SLE\"] = np.square(\n",
    "            np.log1p(train_df[\"predicted\"]) - np.log1p(train_df[\"sales\"])\n",
    "        )\n",
    "        train_df[\"split\"] = \"train\"\n",
    "\n",
    "        test_df = pd.concat([y_test.stack([\"store_nbr\", \"family\"]), y_test_pred.stack([\"store_nbr\", \"family\"])],axis=1)\n",
    "        test_df.columns = [\"sales\", \"predicted\"]\n",
    "        test_df[\"SLE\"] = np.square(\n",
    "            np.log1p(test_df[\"predicted\"]) - np.log1p(test_df[\"sales\"])\n",
    "        )\n",
    "        test_df[\"split\"] = \"test\"\n",
    "\n",
    "        # Compute RMSLE for both train and test\n",
    "        rmsle_train = RMSLE(train_df[\"sales\"], train_df[\"predicted\"])\n",
    "        rmsle_test = RMSLE(test_df[\"sales\"], test_df[\"predicted\"])\n",
    "        rmsle_reports.append((rmsle_train, rmsle_test))\n",
    "\n",
    "        # Concatenate results and append to results list\n",
    "        result_df = pd.concat([train_df, test_df]).reset_index()\n",
    "        results.append(result_df)\n",
    "\n",
    "        print(f\"FOLD: {i}\")\n",
    "        print(f\" Train RMSLE: {rmsle_train}\")\n",
    "        print(f\" Test RMSLE: {rmsle_test}\")\n",
    "    \n",
    "    # Convert to DataFrame the rmsle_reports\n",
    "    df_rmsle = pd.DataFrame(rmsle_reports, columns=['RMSLE_train', 'RMSLE_test'])\n",
    "    print(\"\")\n",
    "    print(f\" Average Train RMSLE: {df_rmsle['RMSLE_train'].mean()}\")\n",
    "    print(f\" Average Test RMSLE: {df_rmsle['RMSLE_test'].mean()}\")\n",
    "\n",
    "    return results, rmsle_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cd8bb6c-be59-4f71-824f-129fc840daef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1\n Train RMSLE: 0.3428391437350007\n Test RMSLE: 0.4006097157114463\nFOLD: 2\n Train RMSLE: 0.37202749149322806\n Test RMSLE: 0.4251332980684654\nFOLD: 3\n Train RMSLE: 0.3111557311968022\n Test RMSLE: 0.3987280663806099\nFOLD: 4\n Train RMSLE: 1.1003787393679654\n Test RMSLE: 1.3539705994407443\nFOLD: 5\n Train RMSLE: 0.9396662918540035\n Test RMSLE: 0.6643089300850437\nFOLD: 6\n Train RMSLE: 0.8121079239127986\n Test RMSLE: 0.5177957286050113\nFOLD: 7\n Train RMSLE: 0.9400491073771143\n Test RMSLE: 0.9911693380949543\nFOLD: 8\n Train RMSLE: 0.904308109943702\n Test RMSLE: 0.5251585713897807\nFOLD: 9\n Train RMSLE: 0.5956821151054202\n Test RMSLE: 0.5811083247023155\nFOLD: 10\n Train RMSLE: 0.5568356526157304\n Test RMSLE: 0.6391310068828472\nFOLD: 11\n Train RMSLE: 0.451123634783162\n Test RMSLE: 0.5809267845017297\nFOLD: 12\n Train RMSLE: 0.5153094483943136\n Test RMSLE: 0.5993963475123538\nFOLD: 13\n Train RMSLE: 0.5779931157528012\n Test RMSLE: 0.6336906391468273\nFOLD: 14\n Train RMSLE: 0.5603823406264516\n Test RMSLE: 0.5865753968728629\n\n Average Train RMSLE: 0.641418489011321\n Average Test RMSLE: 0.6355501962424995\n"
     ]
    }
   ],
   "source": [
    "# Corro la función implementada\n",
    "results, rmsle_report = cv_predictions(df_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c35cf5-67d8-4f35-b595-726fc60a7697",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Resultados\n",
    "\n",
    "Muy posiblemente que el train tenga mayor error que el test tiene relacion sobre la cantidad de días que está prediciendo en cada uno. Esto se podría explorar a mayor cabalidad, voy a jugar con varios valores en mis funciones arriba: \n",
    "\n",
    "---\n",
    "* Folds = 25\n",
    "* Train = 50 días - Average Train RMSLE: 0.491\n",
    "* Test = 15 días - Average Test RMSLE: 0.756\n",
    "\n",
    "---\n",
    "* Folds = 17\n",
    "* Train = 80 días - Average Train RMSLE: 0.577\n",
    "* Test = 15 días - Average Test RMSLE: 0.788\n",
    "\n",
    "---\n",
    "* Folds = 14\n",
    "* Train = 100 días - Average Train RMSLE: 0.64\n",
    "* Test = 15 días - Average Test RMSLE: 0.63\n",
    "\n",
    "---\n",
    "* Folds = 12\n",
    "* Train = 120 días - Average Train RMSLE: 0.683\n",
    "* Test = 15 días - Average Test RMSLE: 0.712\n",
    "\n",
    "---\n",
    "* Folds = 10\n",
    "* Train = 150 días - Average Train RMSLE: 0.704\n",
    "* Test = 15 días - Average Test RMSLE: 0.665\n",
    "\n",
    "---\n",
    "* Folds = 7\n",
    "* Train = 200 días - Average Train RMSLE: 0.794\n",
    "* Test = 15 días - Average Test RMSLE: 0.765\n",
    "\n",
    "---\n",
    "* Folds = 4\n",
    "* Train = 360 días - Average Train RMSLE: 0.791\n",
    "* Test = 15 días - Average Test RMSLE: 1.12\n",
    "\n",
    "\n",
    "Se observa que la ventana de train inferior a 100 días afecta el test, hay un sweet spot entre 100-150 dias pero luego train más grande solo daña el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a30413d-dbab-497f-92a0-5c9adcc131a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Average Train RMSLE: 0.641418489011321\n Average Test RMSLE: 0.6355501962424995\n"
     ]
    }
   ],
   "source": [
    "df_rmsle = pd.DataFrame(rmsle_report, columns=['RMSLE_train', 'RMSLE_test'])\n",
    "print(f\" Average Train RMSLE: {df_rmsle['RMSLE_train'].mean()}\")\n",
    "print(f\" Average Test RMSLE: {df_rmsle['RMSLE_test'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c347a0b5-0691-4d57-aff8-0c9c81b55ee5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Replicar el submission\n",
    "\n",
    "En el notebook ocupó el año 2017 para armar el modelo final. Que son exactamente 227 periodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af05663e-5a00-44bf-b41d-39def9f66689",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[83]: 227"
     ]
    }
   ],
   "source": [
    "# Cuantos periodos ocupo en el notebook\n",
    "len(store_sales.loc[\"2017\"].index.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06065210-65b7-43d0-b5ce-4b04a1eff3d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSLE: 0.5488001493903649\n"
     ]
    }
   ],
   "source": [
    "# Repliquemos el submission\n",
    "# Comenzamos con el train en el periodo 2017\n",
    "y = (\n",
    "  store_sales.loc[\"2017\"].reset_index().drop(columns=['id', 'onpromotion'])\n",
    "            .set_index([\"store_nbr\", \"family\", \"date\"])\n",
    "            .sort_index()\n",
    "            .unstack([\"store_nbr\", \"family\"])\n",
    ")\n",
    "\n",
    "\n",
    "# Entrenando features\n",
    "fourier = CalendarFourier(freq='M', order=4)\n",
    "dp = DeterministicProcess(\n",
    "    index=y.index,\n",
    "    constant=True,\n",
    "    order=1,\n",
    "    seasonal=True,\n",
    "    additional_terms=[fourier],\n",
    "    drop=True,\n",
    ")\n",
    "X = dp.in_sample()\n",
    "X['NewYear'] = (X.index.dayofyear == 1)\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Calculando el RMSLE train set\n",
    "y_pred = pd.DataFrame(model.predict(X), index=X.index, columns=y.columns)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "train_df = pd.concat([y.stack([\"store_nbr\", \"family\"]), y_pred.stack([\"store_nbr\", \"family\"])],axis=1)\n",
    "train_df.columns = [\"sales\", \"predicted\"]\n",
    "train_df[\"SLE\"] = np.square(np.log1p(train_df[\"predicted\"]) - np.log1p(train_df[\"sales\"]))\n",
    "\n",
    "print(f\"Train RMSLE: {RMSLE(train_df['sales'], train_df['predicted'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3657aaa4-7814-40e3-8cb3-460bbf5d35e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seguimos con el test\n",
    "X_test = dp.out_of_sample(steps=16)\n",
    "X_test.index.name = 'date'\n",
    "X_test['NewYear'] = (X_test.index.dayofyear == 1)\n",
    "\n",
    "y_submit = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\n",
    "y_submit = y_submit.clip(lower=0)\n",
    "y_submit = y_submit.stack(['store_nbr', 'family'])\n",
    "y_submit = y_submit.join(df_test.reset_index().set_index([\"store_nbr\", \"family\", \"date\"]).id).reindex(columns=['id', 'sales'])\n",
    "\n",
    "# Pasamos a spark y guardamos\n",
    "spark_df = spark.createDataFrame(y_submit)\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"analytics_inversiones.fr_kgl_submission\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "151fc13c-b512-49f3-b81a-192b22485bda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Submission con mejoras\n",
    "\n",
    "Mejoras con ventanas más pequeña de 100 días"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26aa8150-091b-494f-9c03-1ccec5400b21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha Inicio: 2017-05-08\nFecha Fin: 2017-08-15\n"
     ]
    }
   ],
   "source": [
    "# Obteniendo los últimos 100 periodos\n",
    "ult_100 = store_sales.index.unique()[-100:]\n",
    "print(f\"Fecha Inicio: {np.min(ult_100)}\")\n",
    "print(f\"Fecha Fin: {np.max(ult_100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202a354a-522d-4da8-84f0-6850aae18bbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSLE: 0.417717979829191\n"
     ]
    }
   ],
   "source": [
    "# Comenzamos con el train en los últimos 100 periodos\n",
    "y = (\n",
    "  store_sales.loc[ult_100].reset_index().drop(columns=['id', 'onpromotion'])\n",
    "            .set_index([\"store_nbr\", \"family\", \"date\"])\n",
    "            .sort_index()\n",
    "            .unstack([\"store_nbr\", \"family\"])\n",
    ")\n",
    "\n",
    "# Entrenando features\n",
    "fourier = CalendarFourier(freq='M', order=4)\n",
    "dp = DeterministicProcess(\n",
    "    index=y.index,\n",
    "    constant=True,\n",
    "    order=1,\n",
    "    seasonal=True,\n",
    "    additional_terms=[fourier],\n",
    "    drop=True,\n",
    ")\n",
    "X = dp.in_sample()\n",
    "X['NewYear'] = (X.index.dayofyear == 1)\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Calculando el RMSLE train set\n",
    "y_pred = pd.DataFrame(model.predict(X), index=X.index, columns=y.columns)\n",
    "y_pred = y_pred.clip(lower=0)\n",
    "train_df = pd.concat([y.stack([\"store_nbr\", \"family\"]), y_pred.stack([\"store_nbr\", \"family\"])],axis=1)\n",
    "train_df.columns = [\"sales\", \"predicted\"]\n",
    "train_df[\"SLE\"] = np.square(np.log1p(train_df[\"predicted\"]) - np.log1p(train_df[\"sales\"]))\n",
    "\n",
    "print(f\"Train RMSLE: {RMSLE(train_df['sales'], train_df['predicted'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a7c790d-2dcc-442e-99cd-ae85970c6de2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Es más bajo el RMSLE como esperabamos dado el CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7040a58-6490-4226-a452-8e2c7be76284",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seguimos con el test\n",
    "X_test = dp.out_of_sample(steps=16)\n",
    "X_test.index.name = 'date'\n",
    "X_test['NewYear'] = (X_test.index.dayofyear == 1)\n",
    "\n",
    "y_submit = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)\n",
    "y_submit = y_submit.clip(lower=0)\n",
    "y_submit = y_submit.stack(['store_nbr', 'family'])\n",
    "y_submit = y_submit.join(df_test.reset_index().set_index([\"store_nbr\", \"family\", \"date\"]).id).reindex(columns=['id', 'sales'])\n",
    "\n",
    "# Pasamos a spark y guardamos\n",
    "spark_df = spark.createDataFrame(y_submit)\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"analytics_inversiones.fr_kgl_submission\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "S5_Replicacion_Tutorial_Proyecto_Seasonality",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
